{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a90c8e24-72e1-4596-9921-aa2dbd450caa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial shape: (8054, 153)\n",
      "Dropped 24 all-NaN cols, 5 constant cols. New feature count: 100\n",
      "Feature count: 100\n",
      "[Split] Train: (4832, 100), Test: (3222, 100)\n",
      "Positive rate (train/test): 0.501 0.501\n",
      "\n",
      "=== Validation (on 40% of train) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.998448</td>\n",
       "      <td>0.998966</td>\n",
       "      <td>0.997934</td>\n",
       "      <td>0.998450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.992240</td>\n",
       "      <td>0.998953</td>\n",
       "      <td>0.985537</td>\n",
       "      <td>0.992200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.991723</td>\n",
       "      <td>0.992754</td>\n",
       "      <td>0.990702</td>\n",
       "      <td>0.991727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.937403</td>\n",
       "      <td>0.908390</td>\n",
       "      <td>0.973140</td>\n",
       "      <td>0.939651</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall        F1\n",
       "0  Logistic Regression  0.998448   0.998966  0.997934  0.998450\n",
       "1        Random Forest  0.992240   0.998953  0.985537  0.992200\n",
       "2        Decision Tree  0.991723   0.992754  0.990702  0.991727\n",
       "3                  KNN  0.937403   0.908390  0.973140  0.939651"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Held-out Test: Logistic Regression ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9969    0.9994    0.9981      1608\n",
      "           1     0.9994    0.9969    0.9981      1614\n",
      "\n",
      "    accuracy                         0.9981      3222\n",
      "   macro avg     0.9981    0.9981    0.9981      3222\n",
      "weighted avg     0.9981    0.9981    0.9981      3222\n",
      "\n",
      "Confusion matrix (labels 0,1):\n",
      " [[1607    1]\n",
      " [   5 1609]]\n"
     ]
    }
   ],
   "source": [
    "# === Setup ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 1) Load data\n",
    "# ------------------------------------------------------\n",
    "csv_path = Path(\"../data/raw/exoplanets_2025.csv\")\n",
    "data = pd.read_csv(csv_path)\n",
    "print(\"Initial shape:\", data.shape)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 2) Targets (no leakage)\n",
    "# ------------------------------------------------------\n",
    "# Binary target: 1 if pipeline labeled CANDIDATE, else 0\n",
    "data[\"ExoplanetCandidate\"] = (data[\"koi_pdisposition\"] == \"CANDIDATE\").astype(int)\n",
    "\n",
    "# Optional multi-level label (not used for prediction here)\n",
    "# 2=CONFIRMED, 1=CANDIDATE, 0=FALSE POSITIVE/OTHER\n",
    "data[\"ExoplanetConfirmed\"] = np.select(\n",
    "    [\n",
    "        data[\"koi_disposition\"] == \"CONFIRMED\",\n",
    "        data[\"koi_disposition\"] == \"CANDIDATE\",\n",
    "    ],\n",
    "    [2, 1],\n",
    "    default=0,\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 3) Columns to drop (IDs, strings, labels)\n",
    "# ------------------------------------------------------\n",
    "drop_cols = [\n",
    "    \"kepler_name\",           # string\n",
    "    \"kepoi_name\",            # string\n",
    "    \"koi_disposition\",       # label text (drop)\n",
    "    \"koi_pdisposition\",      # label text (drop)\n",
    "    \"ExoplanetCandidate\",    # drop from X later via column selection to be safe\n",
    "    \"ExoplanetConfirmed\",    # drop from X later to avoid leakage\n",
    "    \"koi_tce_delivname\",     # string delivery tag\n",
    "]\n",
    "\n",
    "# ID / weakly-informative\n",
    "maybe_drop_cols = [\n",
    "    \"kepid\",       # identifier\n",
    "    \"ra\", \"dec\",   # sky coordinates (optional, drop to simplify)\n",
    "    \"koi_kepmag\",  # can keep or drop; keeping is fine; drop if you want only physics-ish\n",
    "]\n",
    "\n",
    "drop_cols = [c for c in drop_cols if c in data.columns]\n",
    "maybe_drop_cols = [c for c in maybe_drop_cols if c in data.columns]\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 4) Basic cleaning: numeric-only features or encode categoricals\n",
    "#    (Here we choose \"numeric-only\" for simplicity)\n",
    "# ------------------------------------------------------\n",
    "# Build candidate X with numeric columns only\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove the targets from numeric feature list if present\n",
    "for t in [\"ExoplanetCandidate\", \"ExoplanetConfirmed\"]:\n",
    "    if t in numeric_cols:\n",
    "        numeric_cols.remove(t)\n",
    "\n",
    "# Optionally ensure we don't include IDs / coords\n",
    "for c in maybe_drop_cols:\n",
    "    if c in numeric_cols:\n",
    "        numeric_cols.remove(c)\n",
    "\n",
    "X_all = data[numeric_cols].copy()\n",
    "y_all = data[\"ExoplanetCandidate\"].copy()\n",
    "\n",
    "# Fill numeric NaNs with median (fit only on train later via pipeline if desired)\n",
    "# Make sure weird values won't break the scaler/estimators\n",
    "X_all = X_all.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Drop columns that are entirely NaN or constant (those break scaling/learning)\n",
    "all_nan_cols = X_all.columns[X_all.isna().all()]\n",
    "if len(all_nan_cols):\n",
    "    X_all = X_all.drop(columns=all_nan_cols)\n",
    "\n",
    "const_cols = X_all.columns[X_all.nunique(dropna=True) <= 1]\n",
    "if len(const_cols):\n",
    "    X_all = X_all.drop(columns=const_cols)\n",
    "\n",
    "print(f\"Dropped {len(all_nan_cols)} all-NaN cols, {len(const_cols)} constant cols. New feature count: {X_all.shape[1]}\")\n",
    "\n",
    "\n",
    "print(f\"Feature count: {X_all.shape[1]}\")\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 5) Train / test split (outer held-out)\n",
    "# ------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_all, y_all, test_size=0.4, random_state=1, stratify=y_all\n",
    ")\n",
    "\n",
    "print(f\"[Split] Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(\"Positive rate (train/test):\", y_train.mean().round(3), y_test.mean().round(3))\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 6) Define models with appropriate preprocessing\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# For linear / distance-based models: impute -> scale\n",
    "scale_pipeline = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\", StandardScaler(with_mean=True, with_std=True)),\n",
    "])\n",
    "\n",
    "# For tree/ensemble models: impute only (no scaling)\n",
    "impute_only = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "])\n",
    "\n",
    "lr_pipe = Pipeline([\n",
    "    (\"prep\", scale_pipeline),\n",
    "    (\"clf\", LogisticRegression(max_iter=5000, class_weight=\"balanced\", random_state=1)),\n",
    "])\n",
    "\n",
    "knn_pipe = Pipeline([\n",
    "    (\"prep\", scale_pipeline),\n",
    "    (\"clf\", KNeighborsClassifier(n_neighbors=5, metric=\"manhattan\")),\n",
    "])\n",
    "\n",
    "dt_pipe = Pipeline([\n",
    "    (\"prep\", impute_only),\n",
    "    (\"clf\", DecisionTreeClassifier(random_state=1, class_weight=\"balanced\")),\n",
    "])\n",
    "\n",
    "rf_pipe = Pipeline([\n",
    "    (\"prep\", impute_only),\n",
    "    (\"clf\", RandomForestClassifier(n_estimators=300, random_state=1, class_weight=\"balanced\", n_jobs=-1)),\n",
    "])\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": lr_pipe,\n",
    "    \"KNN\": knn_pipe,\n",
    "    \"Decision Tree\": dt_pipe,\n",
    "    \"Random Forest\": rf_pipe,\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 7) Fit on TRAIN; evaluate on a validation split or via simple holdout (we'll use a small inner split)\n",
    "#    If you want a real CV, swap this for StratifiedKFold cross_val_score.\n",
    "# ------------------------------------------------------\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.4, random_state=1, stratify=y_train\n",
    ")\n",
    "\n",
    "def eval_model(name, model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    return {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y, y_pred),\n",
    "        \"Precision\": precision_score(y, y_pred, zero_division=0),\n",
    "        \"Recall\": recall_score(y, y_pred, zero_division=0),\n",
    "        \"F1\": f1_score(y, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "val_rows = []\n",
    "for name, pipe in models.items():\n",
    "    pipe.fit(X_tr, y_tr)\n",
    "    val_rows.append(eval_model(name, pipe, X_val, y_val))\n",
    "\n",
    "val_df = pd.DataFrame(val_rows).sort_values(\"F1\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\n=== Validation (on 40% of train) ===\")\n",
    "display(val_df)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# 8) Pick the best (by F1), refit on full TRAIN, and evaluate on held-out TEST\n",
    "# ------------------------------------------------------\n",
    "best_name = val_df.iloc[0][\"Model\"]\n",
    "best_pipe = models[best_name]\n",
    "best_pipe.fit(X_train, y_train)\n",
    "\n",
    "y_pred_test = best_pipe.predict(X_test)\n",
    "\n",
    "print(f\"\\n=== Held-out Test: {best_name} ===\")\n",
    "print(classification_report(y_test, y_pred_test, digits=4))\n",
    "cm = confusion_matrix(y_test, y_pred_test, labels=[0,1])\n",
    "print(\"Confusion matrix (labels 0,1):\\n\", cm)\n",
    "\n",
    "# Optional: save splits and predictions for your notebook files\n",
    "out_dir = Path(\"notebooks\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "pd.concat([X_train, y_train.rename(\"ExoplanetCandidate\")], axis=1).to_csv(out_dir/\"train_baseline.csv\", index=False)\n",
    "X_test.to_csv(out_dir/\"test_baseline.csv\", index=False)\n",
    "pd.Series(y_test, name=\"ExoplanetCandidate\").to_csv(out_dir/\"test_solution_baseline.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fbeab13-a5c5-489c-8e85-8deba932064c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped koi_score (pipeline confidence, avoids leakage)\n",
      "Baseline features: 100\n",
      "Improved features: 95\n"
     ]
    }
   ],
   "source": [
    "# Remove the KOI pipeline's built-in false-positive flags\n",
    "flags_to_drop = [\"koi_fpflag_nt\", \"koi_fpflag_ss\", \"koi_fpflag_co\", \"koi_fpflag_ec\"]\n",
    "X_all_improved = X_all.drop(columns=[c for c in flags_to_drop if c in X_all.columns], errors=\"ignore\")\n",
    "\n",
    "# Drop koi_score (pipeline confidence score — too correlated with label, leaks info)\n",
    "if \"koi_score\" in X_all_improved.columns:\n",
    "    X_all_improved = X_all_improved.drop(columns=[\"koi_score\"])\n",
    "    print(\"Dropped koi_score (pipeline confidence, avoids leakage)\")\n",
    "\n",
    "print(\"Baseline features:\", X_all.shape[1])\n",
    "print(\"Improved features:\", X_all_improved.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "198c6c41-9d41-4dac-a9cf-9aa86d53c148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Validation (Improved) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.917227</td>\n",
       "      <td>0.927061</td>\n",
       "      <td>0.905992</td>\n",
       "      <td>0.916405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.895499</td>\n",
       "      <td>0.883000</td>\n",
       "      <td>0.912190</td>\n",
       "      <td>0.897358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.872802</td>\n",
       "      <td>0.871901</td>\n",
       "      <td>0.872351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.861873</td>\n",
       "      <td>0.813226</td>\n",
       "      <td>0.940083</td>\n",
       "      <td>0.872065</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  Precision    Recall        F1\n",
       "0        Random Forest  0.917227   0.927061  0.905992  0.916405\n",
       "1  Logistic Regression  0.895499   0.883000  0.912190  0.897358\n",
       "2        Decision Tree  0.872219   0.872802  0.871901  0.872351\n",
       "3                  KNN  0.861873   0.813226  0.940083  0.872065"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Held-out Test (Improved): Random Forest ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9186    0.9198    0.9192      1608\n",
      "           1     0.9200    0.9188    0.9194      1614\n",
      "\n",
      "    accuracy                         0.9193      3222\n",
      "   macro avg     0.9193    0.9193    0.9193      3222\n",
      "weighted avg     0.9193    0.9193    0.9193      3222\n",
      "\n",
      "Confusion matrix (labels 0,1):\n",
      " [[1479  129]\n",
      " [ 131 1483]]\n",
      "✅ Improved results saved to notebooks/ (CSV, TXT, JSON)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------\n",
    "# Improved Model Training & Evaluation\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# 1) Train / test split (outer held-out)\n",
    "X_train_imp, X_test_imp, y_train_imp, y_test_imp = train_test_split(\n",
    "    X_all_improved, y_all, test_size=0.4, random_state=1, stratify=y_all\n",
    ")\n",
    "\n",
    "# 2) Validation split (inner)\n",
    "X_tr_imp, X_val_imp, y_tr_imp, y_val_imp = train_test_split(\n",
    "    X_train_imp, y_train_imp, test_size=0.4, random_state=1, stratify=y_train_imp\n",
    ")\n",
    "\n",
    "# 3) Train all models on improved dataset\n",
    "val_rows_imp = []\n",
    "for name, pipe in models.items():\n",
    "    pipe.fit(X_tr_imp, y_tr_imp)\n",
    "    val_rows_imp.append(eval_model(name, pipe, X_val_imp, y_val_imp))\n",
    "\n",
    "val_df_imp = pd.DataFrame(val_rows_imp).sort_values(\"F1\", ascending=False).reset_index(drop=True)\n",
    "print(\"\\n=== Validation (Improved) ===\")\n",
    "display(val_df_imp)\n",
    "\n",
    "# 4) Best model on improved dataset\n",
    "best_name_imp = val_df_imp.iloc[0][\"Model\"]\n",
    "best_pipe_imp = models[best_name_imp]\n",
    "best_pipe_imp.fit(X_train_imp, y_train_imp)\n",
    "\n",
    "y_pred_test_imp = best_pipe_imp.predict(X_test_imp)\n",
    "\n",
    "print(f\"\\n=== Held-out Test (Improved): {best_name_imp} ===\")\n",
    "print(classification_report(y_test_imp, y_pred_test_imp, digits=4))\n",
    "cm_imp = confusion_matrix(y_test_imp, y_pred_test_imp, labels=[0,1])\n",
    "print(\"Confusion matrix (labels 0,1):\\n\", cm_imp)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Save Improved Results\n",
    "# ------------------------------------------------------\n",
    "# 1) Validation results\n",
    "val_df_imp.to_csv(\"notebooks/validation_results_improved.csv\", index=False)\n",
    "\n",
    "# 2) Held-out classification report\n",
    "report_txt_imp = classification_report(y_test_imp, y_pred_test_imp, digits=4)\n",
    "with open(\"notebooks/test_report_improved.txt\", \"w\") as f:\n",
    "    f.write(report_txt_imp)\n",
    "\n",
    "# Also save as JSON\n",
    "report_dict_imp = classification_report(y_test_imp, y_pred_test_imp, digits=4, output_dict=True)\n",
    "\n",
    "# 3) Confusion matrix\n",
    "cm_df_imp = pd.DataFrame(cm_imp, index=[\"Actual 0\",\"Actual 1\"], columns=[\"Pred 0\",\"Pred 1\"])\n",
    "cm_df_imp.to_csv(\"notebooks/confusion_matrix_improved.csv\")\n",
    "\n",
    "# Combine everything into one JSON file\n",
    "results_imp = {\n",
    "    \"validation\": val_df_imp.to_dict(orient=\"records\"),\n",
    "    \"best_model\": best_name_imp,\n",
    "    \"classification_report\": report_dict_imp,\n",
    "    \"confusion_matrix\": cm_df_imp.to_dict()\n",
    "}\n",
    "\n",
    "with open(\"notebooks/improved_results.json\", \"w\") as f:\n",
    "    json.dump(results_imp, f, indent=2)\n",
    "\n",
    "print(\"✅ Improved results saved to notebooks/ (CSV, TXT, JSON)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77cd430b-80b2-459e-9f2f-a4347a1f43da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "koi_disposition counts:\n",
      " koi_disposition\n",
      "FALSE POSITIVE    3965\n",
      "CONFIRMED         2729\n",
      "CANDIDATE         1360\n",
      "Name: count, dtype: int64\n",
      "\n",
      "koi_pdisposition counts:\n",
      " koi_pdisposition\n",
      "CANDIDATE         4034\n",
      "FALSE POSITIVE    4020\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Binary target (ExoplanetCandidate) value counts:\n",
      " ExoplanetCandidate\n",
      "1    4034\n",
      "0    4020\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Cross-tab of koi_pdisposition vs ExoplanetCandidate:\n",
      "ExoplanetCandidate     0     1\n",
      "koi_pdisposition              \n",
      "CANDIDATE              0  4034\n",
      "FALSE POSITIVE      4020     0\n"
     ]
    }
   ],
   "source": [
    "print(\"koi_disposition counts:\\n\", data[\"koi_disposition\"].value_counts(dropna=False))\n",
    "print(\"\\nkoi_pdisposition counts:\\n\", data[\"koi_pdisposition\"].value_counts(dropna=False))\n",
    "print(\"\\nBinary target (ExoplanetCandidate) value counts:\\n\", y_all.value_counts())\n",
    "\n",
    "# How pipeline disposition maps into your binary target\n",
    "print(\"\\nCross-tab of koi_pdisposition vs ExoplanetCandidate:\")\n",
    "print(pd.crosstab(data[\"koi_pdisposition\"], data[\"ExoplanetCandidate\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9abc1e2d-c357-4412-b011-25f5c1b41c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 features by NaN rate:\n",
      "\n",
      "koi_fwm_stat_sig      0.088403\n",
      "koi_zmag              0.062453\n",
      "koi_fwm_prao          0.058356\n",
      "koi_fwm_prao_err      0.058356\n",
      "koi_fwm_pdeco_err     0.056742\n",
      "koi_fwm_pdeco         0.056742\n",
      "koi_dicco_msky_err    0.023715\n",
      "koi_dicco_mra_err     0.023715\n",
      "koi_dicco_mra         0.023715\n",
      "koi_dicco_msky        0.023715\n",
      "dtype: float64\n",
      "\n",
      "Any +/- inf in X_all_improved?: False\n",
      "\n",
      "Columns entirely NaN: []\n"
     ]
    }
   ],
   "source": [
    "# Step 2 — Residual missingness & weird values check\n",
    "\n",
    "# 1) NaN percentage per feature\n",
    "na_rate = X_all_improved.isna().mean().sort_values(ascending=False)\n",
    "print(\"Top 10 features by NaN rate:\\n\")\n",
    "print(na_rate.head(10))\n",
    "\n",
    "# 2) Any infinities left? (should be none, we replaced them earlier)\n",
    "has_inf = np.isinf(X_all_improved.to_numpy()).any()\n",
    "print(\"\\nAny +/- inf in X_all_improved?:\", has_inf)\n",
    "\n",
    "# 3) Columns with 100% missing (shouldn't exist after cleaning)\n",
    "all_nan_cols = X_all_improved.columns[X_all_improved.isna().all()]\n",
    "print(\"\\nColumns entirely NaN:\", list(all_nan_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51dacaa9-60fe-4259-8cc7-af8cc2c8eb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 |correlation| with target:\n",
      "\n",
      "koi_count           0.378026\n",
      "koi_dicco_msky      0.371521\n",
      "koi_smet_err2       0.370637\n",
      "koi_dikco_msky      0.368971\n",
      "koi_steff_err1      0.363041\n",
      "koi_incl            0.354289\n",
      "koi_smet_err1       0.345000\n",
      "koi_steff_err2      0.327367\n",
      "koi_teq             0.306543\n",
      "koi_fwm_stat_sig    0.305555\n",
      "koi_depth           0.297551\n",
      "koi_num_transits    0.292487\n",
      "koi_smass_err1      0.289274\n",
      "koi_smet            0.280806\n",
      "koi_model_snr       0.265737\n",
      "Name: ExoplanetCandidate, dtype: float64\n",
      "\n",
      "⚠️ Suspicious features with |corr| > 0.98:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Step 3 — Correlation with the target (sanity check for leakage)\n",
    "\n",
    "# Compute correlation of every feature with the binary target\n",
    "corr = pd.concat([X_all_improved, y_all.rename(\"ExoplanetCandidate\")], axis=1)\\\n",
    "          .corr(numeric_only=True)[\"ExoplanetCandidate\"]\n",
    "\n",
    "# Drop the target itself\n",
    "corr = corr.drop(labels=[\"ExoplanetCandidate\"])\n",
    "\n",
    "# Sort by absolute correlation\n",
    "corr_sorted = corr.abs().sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 15 |correlation| with target:\\n\")\n",
    "print(corr_sorted.head(15))\n",
    "\n",
    "# Flag suspicious features (very high correlation)\n",
    "suspicious = corr_sorted[corr_sorted > 0.98]\n",
    "print(\"\\n⚠️ Suspicious features with |corr| > 0.98:\")\n",
    "print(list(suspicious.index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d00ea11-b36e-4ecc-82bd-d119b7813a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 15 features by permutation importance (mean ΔF1 when shuffled):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "koi_dikco_msky      0.022729\n",
       "koi_fwm_stat_sig    0.001497\n",
       "koi_dicco_msky      0.000929\n",
       "koi_srho_err1       0.000516\n",
       "koi_model_snr       0.000465\n",
       "koi_fwm_pdeco       0.000413\n",
       "koi_max_mult_ev     0.000310\n",
       "koi_num_transits    0.000258\n",
       "koi_prad            0.000207\n",
       "koi_dor             0.000207\n",
       "koi_ror             0.000103\n",
       "koi_prad_err2       0.000052\n",
       "koi_smass_err2      0.000000\n",
       "koi_srad            0.000000\n",
       "koi_fwm_prao        0.000000\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 4 — Permutation importance for improved best model (Random Forest)\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Make sure we’re using the improved Random Forest model\n",
    "assert best_name_imp == \"Random Forest\", \"Best improved model is not Random Forest — check results first.\"\n",
    "rf_imp = best_pipe_imp\n",
    "\n",
    "# Compute permutation importance on the validation set\n",
    "perm = permutation_importance(\n",
    "    rf_imp, X_val_imp, y_val_imp,\n",
    "    scoring=\"f1\", n_repeats=10, random_state=1, n_jobs=-1\n",
    ")\n",
    "\n",
    "pi = pd.Series(perm.importances_mean, index=X_val_imp.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 15 features by permutation importance (mean ΔF1 when shuffled):\")\n",
    "display(pi.head(15))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
